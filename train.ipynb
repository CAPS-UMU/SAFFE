{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 17:44:11.316970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#last update march 6 8pm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "#%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "#set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.types import _size\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "## Audio\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "\n",
    "####image\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel , CLIPVisionModelWithProjection\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "from transformers import AutoTokenizer, CLIPTextModel,AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model import Transformer\n",
    "#from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "#import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import Audio\n",
    "from datasets import Dataset, Image\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport config\\n# config the database  \\ndef get_ds():\\n    filepathlabel='/home_nfs/mranga/imagenet_100/archive/Labels.json'\\n    filepatht1='/home_nfs/mranga/imagenet_100/rchive/train.X1'\\n    filepatht2='/home_nfs/mranga/imagenet_100/archive/train.X2'\\n    filepatht3='/home_nfs/mranga/imagenet_100/archive/train.X3'\\n    filepatht4='/home_nfs/mranga/imagenet_100/archive/train.X4'\\n    #filepathval='/home_nfs/mranga/imagenet_100/imagenet_100/archive/train.X1'\\n    with open(filepathlabel, 'r') as f:\\n         labels = json.load(f)\\n\\n    images = []\\n\\n    for label in os.listdir('/home_nfs/mranga/imagenet_100/archive/train.X1'):\\n        for filename in os.listdir(f'/home_nfs/mranga/imagenet_100/archive/train.X1/{label}'):\\n             images.append({\\n                 'path': f'/home_nfs/mranga/imagenet_100/archive/train.X1/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n    for label in os.listdir('/home_nfs/mranga/imagenet_100/archive/train.X2'):\\n        for filename in os.listdir(f'/home_nfs/mranga/imagenet_100/archive/train.X2/{label}'):\\n             images.append({\\n                 'path': f'/home_nfs/mranga/imagenet_100/archive/train.X2/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n    for label in os.listdir('/home_nfs/mranga/imagenet_100/archive/train.X3'):\\n        for filename in os.listdir(f'/home_nfs/mranga/imagenet_100/archive/train.X3/{label}'):\\n             images.append({\\n                 'path': f'/home_nfs/mranga/imagenet_100/archive/train.X3/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n    for label in os.listdir('/home_nfs/mranga/imagenet_100/archive/train.X4'):\\n        for filename in os.listdir(f'/home_nfs/mranga/imagenet_100/archive/train.X4/{label}'):\\n              images.append({\\n                 'path': f'/home_nfs/mranga/imagenet_100/archive/train.X4/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n\\n    train_dataloader = pd.DataFrame(images)\\n    \\n    val_images = []\\n\\n    for label in os.listdir('/home_nfs/mranga/imagenet_100/archive/val.X'):\\n        for filename in os.listdir(f'/home_nfs/mranga/imagenet_100/archive/val.X/{label}'):\\n             val_images.append({\\n                 'path': f'/home_nfs/mranga/imagenet_100/archive/val.X/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n              })\\n\\n    val_dataloader = pd.DataFrame(val_images)\\n\\n\\n    #train_ds_size=int(0.9*len(dataset))\\n    #val_ds_size= len(dataset) - train_ds_size\\n    #train_dataset_both ,val_dataset_both = random_split(dataset ,[train_ds_size, val_ds_size])\\n\\n    train_dataloader = DataLoader(images ,25 , shuffle=True)\\n    val_dataloader = DataLoader(val_images , batch_size=1, shuffle=True)\\n    \\n    return train_dataloader,val_dataloader,labels\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import config\n",
    "# config the database  \n",
    "def get_ds():\n",
    "    #path= path for the imagenet dataset\n",
    "    filepathlabel='/path/magenet_100/archive/Labels.json'\n",
    "   ## filepatht1='/home_nfs/mranga/imagenet_100/rchive/train.X1'\n",
    "    #filepatht2='/home_nfs/mranga/imagenet_100/archive/train.X2'\n",
    "   # filepatht3='/home_nfs/mranga/imagenet_100/archive/train.X3'\n",
    "   # filepatht4='/home_nfs/mranga/imagenet_100/archive/train.X4'\n",
    "    #filepathval='/media/maithri/ee0c61c0-1e77-4746-ae07-ddc0cd940859/dataset/imagenet_100/archive/train.X1'\n",
    "    with open(filepathlabel, 'r') as f:\n",
    "         labels = json.load(f)\n",
    "    #print(labels)\n",
    "    images = []\n",
    "    #/media/maithri/ee0c61c0-1e77-4746-ae07-ddc0cd940859/dataset/imagenet_100\n",
    "    for label in os.listdir('/path/imagenet_100/archive/train.X1'):\n",
    "        for filename in os.listdir(f'/path/imagenet_100/archive/train.X1/{label}'):\n",
    "             images.append({\n",
    "                 'path': f'/path/magenet_100/archive/train.X1/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "    for label in os.listdir('/path/imagenet_100/archive/train.X2'):\n",
    "        for filename in os.listdir(f'/path/imagenet_100/archive/train.X2/{label}'):\n",
    "             images.append({\n",
    "                 'path': f'/path/imagenet_100/archive/train.X2/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "    for label in os.listdir('/path/magenet_100/archive/train.X3'):\n",
    "        for filename in os.listdir(f'/path/magenet_100/archive/train.X3/{label}'):\n",
    "             images.append({\n",
    "                 'path': f'/path/magenet_100/archive/train.X3/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "    for label in os.listdir('/path/imagenet_100/archive/train.X4'):\n",
    "        for filename in os.listdir(f'/path/magenet_100/archive/train.X4/{label}'):\n",
    "              images.append({\n",
    "                 'path': f'/path/imagenet_100/archive/train.X4/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "\n",
    "    train_dataloader = pd.DataFrame(images)\n",
    "    \n",
    "    val_images = []\n",
    "\n",
    "    for label in os.listdir('/path/imagenet_100/archive/val.X'):\n",
    "        for filename in os.listdir(f'/path/dataset/imagenet_100/archive/val.X/{label}'):\n",
    "             val_images.append({\n",
    "                 'path': f'/path/dataset/imagenet_100/archive/val.X/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "              })\n",
    "\n",
    "    val_dataloader = pd.DataFrame(val_images)\n",
    "\n",
    "\n",
    "    #train_ds_size=int(0.9*len(dataset))\n",
    "    #val_ds_size= len(dataset) - train_ds_size\n",
    "    #train_dataset_both ,val_dataset_both = random_split(dataset ,[train_ds_size, val_ds_size])\n",
    "    batch_size=config['batch_size'] \n",
    "    train_dataloader = DataLoader(images ,batch_size , shuffle=True)\n",
    "    val_dataloader = DataLoader(val_images , batch_size=4, shuffle=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageNet100id():\n",
    "    \n",
    "   imagenet_id=[\"n01968897\",\n",
    "    \"n01770081\",\n",
    "    \"n01818515\",\n",
    "    \"n02011460\",\n",
    "    \"n01496331\",\n",
    "    \"n01847000\",\n",
    "    \"n01687978\",\n",
    "    \"n01740131\",\n",
    "    \"n01537544\",\n",
    "    \"n01491361\",\n",
    "    \"n02007558\",\n",
    "    \"n01735189\",\n",
    "    \"n01630670\",\n",
    "    \"n01440764\",\n",
    "    \"n01819313\",\n",
    "    \"n02002556\",\n",
    "    \"n01667778\",\n",
    "    \"n01755581\",\n",
    "    \"n01924916\",\n",
    "    \"n01751748\",\n",
    "    \"n01984695\",\n",
    "    \"n01729977\",\n",
    "    \"n01614925\",\n",
    "    \"n01608432\",\n",
    "    \"n01443537\",\n",
    "    \"n01770393\",\n",
    "    \"n01855672\",\n",
    "    \"n01560419\",\n",
    "    \"n01592084\",\n",
    "    \"n01914609\",\n",
    "    \"n01582220\",\n",
    "    \"n01667114\",\n",
    "    \"n01985128\",\n",
    "    \"n01820546\",\n",
    "    \"n01773797\",\n",
    "    \"n02006656\",\n",
    "    \"n01986214\",\n",
    "    \"n01484850\",\n",
    "    \"n01749939\",\n",
    "    \"n01828970\",\n",
    "    \"n02018795\",\n",
    "    \"n01695060\",\n",
    "    \"n01729322\",\n",
    "    \"n01677366\",\n",
    "    \"n01734418\",\n",
    "    \"n01843383\",\n",
    "    \"n01806143\",\n",
    "    \"n01773549\",\n",
    "    \"n01775062\",\n",
    "    \"n01728572\",\n",
    "    \"n01601694\",\n",
    "    \"n01978287\",\n",
    "    \"n01930112\",\n",
    "    \"n01739381\",\n",
    "    \"n01883070\",\n",
    "    \"n01774384\",\n",
    "    \"n02037110\",\n",
    "    \"n01795545\",\n",
    "    \"n02027492\",\n",
    "    \"n01531178\",\n",
    "    \"n01944390\",\n",
    "    \"n01494475\",\n",
    "    \"n01632458\",\n",
    "    \"n01698640\",\n",
    "    \"n01675722\",\n",
    "    \"n01877812\",\n",
    "    \"n01622779\",\n",
    "    \"n01910747\",\n",
    "    \"n01860187\",\n",
    "    \"n01796340\",\n",
    "    \"n01833805\",\n",
    "    \"n01685808\",\n",
    "    \"n01756291\",\n",
    "    \"n01514859\",\n",
    "    \"n01753488\",\n",
    "    \"n02058221\",\n",
    "    \"n01632777\",\n",
    "    \"n01644900\",\n",
    "    \"n02018207\",\n",
    "    \"n01664065\",\n",
    "    \"n02028035\",\n",
    "    \"n02012849\",\n",
    "    \"n01776313\",\n",
    "    \"n02077923\",\n",
    "    \"n01774750\",\n",
    "    \"n01742172\",\n",
    "    \"n01943899\",\n",
    "    \"n01798484\",\n",
    "    \"n02051845\",\n",
    "    \"n01824575\",\n",
    "    \"n02013706\",\n",
    "    \"n01955084\",\n",
    "    \"n01773157\",\n",
    "    \"n01665541\",\n",
    "    \"n01498041\",\n",
    "    \"n01978455\",\n",
    "    \"n01693334\",\n",
    "    \"n01950731\",\n",
    "    \"n01829413\",\n",
    "    \"n01514668\"]\n",
    "   return imagenet_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saas,sasas,lables=get_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(): \n",
    "    model = Transformer(config[\"d_model\"], config[\"ffn_hidden\"], config[\"num_heads\"], config[\"drop_prob\"], config[\"num_layers\"], config[\"kn_vocab_size\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model():\n",
    "\n",
    "    image_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\",output_hidden_states =True,output_attentions=True)\n",
    "    image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    #image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    return image_model,image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_model():\n",
    "\n",
    "    processor_audio = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "    model_audio = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", output_hidden_states =True,output_attentions=True)\n",
    "    \n",
    "\n",
    "    return model_audio,processor_audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_model():\n",
    "    text_model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\",output_hidden_states =True,output_attentions=True)\n",
    "    text_Tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return text_model ,text_Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_layer():\n",
    "    \n",
    "    image_layerno_1=7\n",
    "    image_layerno_2=6\n",
    "    print(\"image layer number\",image_layerno_1,image_layerno_2)\n",
    "    return image_layerno_1 ,image_layerno_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_validation(model, validation_ds,labels_all, device, print_msg, global_step, writer, num_examples=10):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    image_model,image_processor =get_image_model()\n",
    "    model_audio,processor_audio =get_audio_model()\n",
    "    text_model,text_Tokenizer =get_text_model()\n",
    "    image100id=imageNet100id()\n",
    "    image_layerno_1,image_layerno_2= get_hidden_layer()\n",
    "\n",
    "    source_image = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "    realclass_count=0\n",
    "    number_inputs=0\n",
    "    classid=''\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    #batch_size=config['batch_size'] \n",
    "    batch_size=1\n",
    "   # batch_iterator = tqdm(validation_ds.iter(batch_size))\n",
    "    batch_iterator = tqdm(validation_ds)\n",
    "       # print(\"batch_iterator,batch_iterator\",batch_iterator)\n",
    "        #for index, row in df.iterrows():\n",
    "        #for batch in batch_iterator:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_iterator :\n",
    "            count += 1\n",
    "\n",
    "########################  image KQV extraction \n",
    "\n",
    "            paths=batch[\"path\"]\n",
    "            image_file = Dataset.from_dict({\"image\": paths}).cast_column(\"image\", Image())  \n",
    "            \n",
    "            image_model=image_model.to(device)\n",
    "\n",
    "                \n",
    "            image_1 = image_processor(images=image_file[0][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_outputs_1 = image_model(**image_1)\n",
    "            image_last_hidden_state_1 = image_outputs_1.last_hidden_state\n",
    "            image_pooled_output_1 = image_outputs_1.pooler_output  \n",
    "            image_layear_output_1=image_outputs_1.hidden_states[image_layerno_1],image_outputs_1.hidden_states[image_layerno_2]\n",
    "                  \n",
    "                  \n",
    "            image_2 = image_processor(images=image_file[1][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_2 = image_model(**image_2)\n",
    "            image_last_hidden_state_2 = image_outputs_2.last_hidden_state\n",
    "            image_pooled_output_2 = image_outputs_2.pooler_output  \n",
    "            image_layear_output_2=image_outputs_2.hidden_states[image_layerno_1],image_outputs_2.hidden_states[image_layerno_2]\n",
    "            \n",
    "            \n",
    "            image_3 = image_processor(images=image_file[2][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_3 = image_model(**image_3)\n",
    "            image_last_hidden_state_3 = image_outputs_3.last_hidden_state\n",
    "            image_pooled_output_3 = image_outputs_3.pooler_output  \n",
    "            image_layear_output_3=image_outputs_3.hidden_states[image_layerno_1],image_outputs_3.hidden_states[image_layerno_2]\n",
    "            \n",
    "            image_4 = image_processor(images=image_file[3][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_4 = image_model(**image_4)\n",
    "            image_last_hidden_state_4 = image_outputs_4.last_hidden_state\n",
    "            image_pooled_output_4 = image_outputs_4.pooler_output  \n",
    "            image_layear_output_4=image_outputs_4.hidden_states[image_layerno_1],image_outputs_4.hidden_states[image_layerno_2]\n",
    "\n",
    "\n",
    "            label=batch[\"label\"]\n",
    "          \n",
    "\n",
    "\n",
    "            inputs_text = text_Tokenizer(text=label[0], padding=True, return_tensors=\"pt\").to(device)\n",
    "            text_model= text_model.to(device)\n",
    "            text_outputs = text_model(**inputs_text)\n",
    "            text_pooled_output = text_outputs.pooler_output\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "  \n",
    "#############################################################################\n",
    "            text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "            text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "            text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "            _start = 0\n",
    "            _end = config[\"d_model\"]\n",
    "            text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "            text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "            text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "\n",
    "            #image model parameters \n",
    "            image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "            image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "            image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "            image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "            image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "            image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "\n",
    "            \n",
    "            scores_1 = [None] * 100\n",
    "            scores_2 = [None] * 100\n",
    "            scores_3 = [None] * 100\n",
    "            scores_4 = [None] * 100\n",
    "                  \n",
    "            for x in range(0, 100):\n",
    "                #print(\"text\",labels_all[image100id[x]])\n",
    "                inputs_text = text_Tokenizer(text=labels_all[image100id[x]], padding=True, return_tensors=\"pt\").to(device)\n",
    "                text_model= text_model.to(device)\n",
    "                text_outputs = text_model(**inputs_text)\n",
    "                text_pooled_output = text_outputs.pooler_output\n",
    "\n",
    "                text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "                proj_output_1=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_1, image_pooled_output_1).to(device)\n",
    "                proj_output_1 = proj_output_1.detach().cpu().numpy()\n",
    "                text_pooled_output =text_pooled_output.detach().cpu().numpy()\n",
    "      \n",
    "                scores_1[x] = np.dot(proj_output_1, text_pooled_output.T)/(norm(proj_output_1)*(norm(text_pooled_output))) \n",
    "                    \n",
    "                    \n",
    "                proj_output_2=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_2, image_pooled_output_2).to(device)  \n",
    "                      \n",
    "                    \n",
    "                proj_output_2 = proj_output_2.detach().cpu().numpy()\n",
    "                  \n",
    "                scores_2[x] = np.dot(proj_output_2, text_pooled_output.T)/(norm(proj_output_2)*(norm(text_pooled_output))) \n",
    "                \n",
    "                proj_output_3=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_3, image_pooled_output_3).to(device)  \n",
    "                      \n",
    "                    \n",
    "                proj_output_3 = proj_output_3.detach().cpu().numpy()\n",
    "                  \n",
    "                scores_3[x] = np.dot(proj_output_3, text_pooled_output.T)/(norm(proj_output_3)*(norm(text_pooled_output))) \n",
    "                    #scores_2[x]\n",
    "                    \n",
    "                proj_output_4=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_4, image_pooled_output_4).to(device)  \n",
    "                      \n",
    "                    \n",
    "                proj_output_4 = proj_output_4.detach().cpu().numpy()\n",
    "                  \n",
    "                scores_4[x] = np.dot(proj_output_4, text_pooled_output.T)/(norm(proj_output_4)*(norm(text_pooled_output))) \n",
    "                    \n",
    "                    \n",
    "                  #print(\"real_class\",np.argmax(scores_1),np.argmax(scores_2))\n",
    "            real_class=np.argmax(scores_1),np.argmax(scores_2),np.argmax(scores_3),np.argmax(scores_4)\n",
    "                \n",
    "           # print(\"real_class\",real_class[0],real_class[1],real_class[2],real_class[3])\n",
    "                  #print(\"scores\",np.argmax(appfor, axis=0),label)\n",
    "                \n",
    "              \n",
    "        \n",
    "            classid=str(batch[\"id\"][0]),str(batch[\"id\"][1]),str(batch[\"id\"][2]),str(batch[\"id\"][3])\n",
    "           # print(str(batch[\"id\"][0]),str(batch[\"id\"][1]),str(batch[\"id\"][2]),str(batch[\"id\"][3]))\n",
    "           # print(image100id[real_class[0]],image100id[real_class[1]],image100id[real_class[2]],image100id[real_class[3]])\n",
    "            for x in range(0, 4):\n",
    "              #  print(x)\n",
    "                if classid[x] == image100id[real_class[x]]:\n",
    "                        realclass_count=realclass_count+1\n",
    "                        number_inputs=number_inputs+1\n",
    "                        #print(\"correct\",realclass_count)\n",
    "                else:\n",
    "                        #  print(\"wwwwwwwwwww\",number_inputs)\n",
    "                        number_inputs=number_inputs+1\n",
    "\n",
    "\n",
    "                  \n",
    "\n",
    "                  #source_image.append(scores)\n",
    "            expected.append(1.00)\n",
    "            predicted.append(scores_1)\n",
    "            \n",
    "\n",
    "\n",
    "    Accuracy =(realclass_count / number_inputs*100 )\n",
    "\n",
    "    print(\"Accuracy\", Accuracy,realclass_count,number_inputs, \"%\")\n",
    "    return Accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "   #device=\"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader,val_dataloader,lablesall= get_ds()\n",
    "    image_model,image_processor =get_image_model()\n",
    "    model_audio,processor_audio =get_audio_model()\n",
    "    text_model,text_Tokenizer =get_text_model()\n",
    "    image_layerno_1,image_layerno_2= get_hidden_layer()\n",
    "  \n",
    "    print(\"Training dataset size \", len(train_dataloader )*config['batch_size'] )  \n",
    "\n",
    "    model = get_model().to(device)\n",
    "\n",
    "     # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "\n",
    "   # optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    value_opt = torch.optim.Adam(params = model.parameters(), lr = config['lr'])\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    #loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)   \n",
    "    \n",
    "\n",
    "\n",
    "   # abc=torch.rand(1, 768).to(device)\n",
    "    print(\"Layers \",config[\"num_layers\"])\n",
    "    \n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        #print(\"epoch\",epoch)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        #train_dataset_both ,val_dataset_both ,train_dataloader,val_dataloader= get_ds()\n",
    "        model.train()\n",
    "        batch_size=config['batch_size'] \n",
    "  \n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "              \n",
    "########################  image KQV extraction \n",
    "            \n",
    "\n",
    "            paths=batch[\"path\"]\n",
    "            image_file = Dataset.from_dict({\"image\": paths}).cast_column(\"image\", Image())  \n",
    "            #dataset = Dataset.from_dict({\"image\": [\"path/to/image_1\", \"path/to/image_2\", ..., \"path/to/image_n\"]}).cast_column(\"image\", Image())\n",
    "            #dataset[0][\"image\"]\n",
    "            #print(\"maithri ranga1\",image_file)\n",
    "            iend = config['batch_size'] \n",
    "            image = image_processor(images=image_file[0:iend][\"image\"], return_tensors=\"pt\").to(device)\n",
    "\n",
    "           # image = image_processor(images=batch['path'], return_tensors=\"pt\").to(device)\n",
    "\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs = image_model(**image)\n",
    "            image_last_hidden_state = image_outputs.last_hidden_state\n",
    "            image_pooled_output = image_outputs.pooler_output  \n",
    "           # print(\"image_pooled_output\",image_pooled_output.shape)\n",
    "           # batch_nummber+= 1\n",
    "         \n",
    "            image_layear_output=image_outputs.hidden_states[image_layerno_1],image_outputs.hidden_states[image_layerno_2]\n",
    "\n",
    "            \n",
    "\n",
    "            label=batch[\"label\"]\n",
    "\n",
    "           \n",
    "            #print(\"imageddddddddddddddddddddddd\",label,flabel)\n",
    "            inputs_text = text_Tokenizer(text=label, padding=True, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            text_model= text_model.to(device)\n",
    "            text_outputs = text_model(**inputs_text)\n",
    "            \n",
    "            text_pooled_output = text_outputs.pooler_output\n",
    "           # print(\"imageddddddddddddddddddddddd\",text_pooled_output.shape)\n",
    "            text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "            \n",
    "\n",
    "\n",
    "            #text model parameters \n",
    "            text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "            text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "            text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "            _start = 0\n",
    "            _end = config[\"d_model\"]\n",
    "            text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "            text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "            text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #image model parameters \n",
    "            image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "            image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "            image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "            image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "            image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "            image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "            \n",
    "            #proj_output=model(config['d_model'], config['ffn_hidden'], config['num_heads'], config['drop_prob'], config['num_layers'], config['kn_vocab_size'], model_audio, image_model, outputs_audio, image_outputs)\n",
    "            proj_output=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias,image_key_bias,image_value_bias, image_query_weight,image_key_weight,image_value_weight, \n",
    "                              text_layear_output, image_layear_output,image_pooled_output).to(device)\n",
    "           # out_image = Transformer(audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight, image_model, audio_layear_output, image_layear_output)\n",
    "            text_pooled_output=text_pooled_output/text_pooled_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      \n",
    "            #image_pooled_output=torch.squeeze(image_pooled_output,dim=0).to(device)\n",
    "  \n",
    "            proj_output=proj_output/proj_output.norm(dim=-1, keepdim=True)\n",
    "            image_last_hidden_state=image_last_hidden_state/image_last_hidden_state.norm(dim=-1, keepdim=True)\n",
    "            image_pooled_output=image_pooled_output/image_pooled_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1,768), text_pooled_output .view(-1,768))   # compare with pooled_output image to audio\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        \n",
    "            global_step += 1\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataset_both ,val_dataset_both,labels_all = get_ds()\n",
    "print(\"Layers \",config[\"num_layers\"])\n",
    "print(\"Training dataset size \", len(val_dataset_both ))\n",
    "#val_dataset_both2 =val_dataset_both.shard(num_shards=2, index=0)\n",
    "model = get_model().to(device)\n",
    "Accuracy = []\n",
    "for number in range(0, 40, 1):\n",
    "    if number < 10: \n",
    "        str(number)\n",
    "        number= \"0\"+ str(number)\n",
    "    else:\n",
    "        number=str(number)\n",
    "    cwd2 = \"tmodel_\" + number + \".pt\"\n",
    "    cwd = \"weights\"\n",
    "    os.path.join(cwd,cwd2)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    model_filename =os.path.join(cwd,cwd2)\n",
    "    print(model_filename)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    tAccuracy = run_validation(model, val_dataset_both,labels_all, device,  lambda msg: print(msg), 0, None, num_examples=2500)\n",
    "    Accuracy.append(tAccuracy)\n",
    "    print(\"Merge\",tAccuracy)\n",
    "    with open('output.txt', 'a') as testwritefile:\n",
    "        testwritefile.write(str(number))\n",
    "        testwritefile.write(\"layer accuracy \")\n",
    "        testwritefile.write(str(tAccuracy ))\n",
    "        testwritefile.write(\"\\n \")\n",
    "    print(Accuracy)\n",
    "max_value_index = np.argmax(Accuracy)\n",
    "Maximum_Accuracy = Accuracy[max_value_index]\n",
    "print(\"Maximum_Accuracy:\", Maximum_Accuracy ,\"Maximum Accuracy epoch:\",max_value_index )\n",
    "with open('output.txt', 'a') as testwritefile:\n",
    "        testwritefile.write(\"Maximum_Accuracy:\")\n",
    "        testwritefile.write(str(Maximum_Accuracy))\n",
    "        testwritefile.write(\"Maximum Accuracy epoch:\")\n",
    "        testwritefile.write(str(max_value_index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
