{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#last update march 6 8pm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "#%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "#set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.types import _size\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "## Audio\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "\n",
    "####image\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel , CLIPVisionModelWithProjection\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "from transformers import AutoTokenizer, CLIPTextModel,AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model import Transformer\n",
    "#from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "#import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import Audio\n",
    "from datasets import Dataset, Image\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport config\\n# config the database  \\ndef get_ds():\\n    #path= path for the imagenet dataset\\n    filepathlabel='/path/magenet_100/archive/Labels.json'\\n   ## filepatht1='/home_nfs/mranga/imagenet_100/rchive/train.X1'\\n    #filepatht2='/home_nfs/mranga/imagenet_100/archive/train.X2'\\n   # filepatht3='/home_nfs/mranga/imagenet_100/archive/train.X3'\\n   # filepatht4='/home_nfs/mranga/imagenet_100/archive/train.X4'\\n    #filepathval='/media/maithri/ee0c61c0-1e77-4746-ae07-ddc0cd940859/dataset/imagenet_100/archive/train.X1'\\n    with open(filepathlabel, 'r') as f:\\n         labels = json.load(f)\\n    #print(labels)\\n    images = []\\n    #/media/maithri/ee0c61c0-1e77-4746-ae07-ddc0cd940859/dataset/imagenet_100\\n    for label in os.listdir('/path/imagenet_100/archive/train.X1'):\\n        for filename in os.listdir(f'/path/imagenet_100/archive/train.X1/{label}'):\\n             images.append({\\n                 'path': f'/path/magenet_100/archive/train.X1/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n    for label in os.listdir('/path/imagenet_100/archive/train.X2'):\\n        for filename in os.listdir(f'/path/imagenet_100/archive/train.X2/{label}'):\\n             images.append({\\n                 'path': f'/path/imagenet_100/archive/train.X2/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n    for label in os.listdir('/path/magenet_100/archive/train.X3'):\\n        for filename in os.listdir(f'/path/magenet_100/archive/train.X3/{label}'):\\n             images.append({\\n                 'path': f'/path/magenet_100/archive/train.X3/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n    for label in os.listdir('/path/imagenet_100/archive/train.X4'):\\n        for filename in os.listdir(f'/path/magenet_100/archive/train.X4/{label}'):\\n              images.append({\\n                 'path': f'/path/imagenet_100/archive/train.X4/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n             })\\n\\n    train_dataloader = pd.DataFrame(images)\\n    \\n    val_images = []\\n\\n    for label in os.listdir('/path/imagenet_100/archive/val.X'):\\n        for filename in os.listdir(f'/path/dataset/imagenet_100/archive/val.X/{label}'):\\n             val_images.append({\\n                 'path': f'/path/dataset/imagenet_100/archive/val.X/{label}/{filename}',\\n                 'label': labels[label],\\n                 'id': label,\\n              })\\n\\n    val_dataloader = pd.DataFrame(val_images)\\n\\n\\n    #train_ds_size=int(0.9*len(dataset))\\n    #val_ds_size= len(dataset) - train_ds_size\\n    #train_dataset_both ,val_dataset_both = random_split(dataset ,[train_ds_size, val_ds_size])\\n    batch_size=config['batch_size'] \\n    train_dataloader = DataLoader(images ,batch_size , shuffle=True)\\n    val_dataloader = DataLoader(val_images , batch_size=4, shuffle=True)\\n    \\n    return train_dataloader,val_dataloader,labels\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import config\n",
    "# config the database  \n",
    "def get_ds():\n",
    "    #path= path for the imagenet dataset\n",
    "    filepathlabel='/path/magenet_100/archive/Labels.json'\n",
    "\n",
    "    with open(filepathlabel, 'r') as f:\n",
    "         labels = json.load(f)\n",
    "    #print(labels)\n",
    "    images = []\n",
    "    #/media/maithri/ee0c61c0-1e77-4746-ae07-ddc0cd940859/dataset/imagenet_100\n",
    "    for label in os.listdir('/path/imagenet_100/archive/train.X1'):\n",
    "        for filename in os.listdir(f'/path/imagenet_100/archive/train.X1/{label}'):\n",
    "             images.append({\n",
    "                 'path': f'/path/magenet_100/archive/train.X1/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "    for label in os.listdir('/path/imagenet_100/archive/train.X2'):\n",
    "        for filename in os.listdir(f'/path/imagenet_100/archive/train.X2/{label}'):\n",
    "             images.append({\n",
    "                 'path': f'/path/imagenet_100/archive/train.X2/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "    for label in os.listdir('/path/magenet_100/archive/train.X3'):\n",
    "        for filename in os.listdir(f'/path/magenet_100/archive/train.X3/{label}'):\n",
    "             images.append({\n",
    "                 'path': f'/path/magenet_100/archive/train.X3/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "    for label in os.listdir('/path/imagenet_100/archive/train.X4'):\n",
    "        for filename in os.listdir(f'/path/magenet_100/archive/train.X4/{label}'):\n",
    "              images.append({\n",
    "                 'path': f'/path/imagenet_100/archive/train.X4/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "             })\n",
    "\n",
    "    train_dataloader = pd.DataFrame(images)\n",
    "    \n",
    "    val_images = []\n",
    "\n",
    "    for label in os.listdir('/path/imagenet_100/archive/val.X'):\n",
    "        for filename in os.listdir(f'/path/dataset/imagenet_100/archive/val.X/{label}'):\n",
    "             val_images.append({\n",
    "                 'path': f'/path/dataset/imagenet_100/archive/val.X/{label}/{filename}',\n",
    "                 'label': labels[label],\n",
    "                 'id': label,\n",
    "              })\n",
    "\n",
    "    val_dataloader = pd.DataFrame(val_images)\n",
    "\n",
    "\n",
    "    #train_ds_size=int(0.9*len(dataset))\n",
    "    #val_ds_size= len(dataset) - train_ds_size\n",
    "    #train_dataset_both ,val_dataset_both = random_split(dataset ,[train_ds_size, val_ds_size])\n",
    "    batch_size=config['batch_size'] \n",
    "    train_dataloader = DataLoader(images ,batch_size , shuffle=True)\n",
    "    val_dataloader = DataLoader(val_images , batch_size=4, shuffle=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageNet100id():\n",
    "    \n",
    "   imagenet_id=[\"n01968897\",\n",
    "    \"n01770081\",\n",
    "    \"n01818515\",\n",
    "    \"n02011460\",\n",
    "    \"n01496331\",\n",
    "    \"n01847000\",\n",
    "    \"n01687978\",\n",
    "    \"n01740131\",\n",
    "    \"n01537544\",\n",
    "    \"n01491361\",\n",
    "    \"n02007558\",\n",
    "    \"n01735189\",\n",
    "    \"n01630670\",\n",
    "    \"n01440764\",\n",
    "    \"n01819313\",\n",
    "    \"n02002556\",\n",
    "    \"n01667778\",\n",
    "    \"n01755581\",\n",
    "    \"n01924916\",\n",
    "    \"n01751748\",\n",
    "    \"n01984695\",\n",
    "    \"n01729977\",\n",
    "    \"n01614925\",\n",
    "    \"n01608432\",\n",
    "    \"n01443537\",\n",
    "    \"n01770393\",\n",
    "    \"n01855672\",\n",
    "    \"n01560419\",\n",
    "    \"n01592084\",\n",
    "    \"n01914609\",\n",
    "    \"n01582220\",\n",
    "    \"n01667114\",\n",
    "    \"n01985128\",\n",
    "    \"n01820546\",\n",
    "    \"n01773797\",\n",
    "    \"n02006656\",\n",
    "    \"n01986214\",\n",
    "    \"n01484850\",\n",
    "    \"n01749939\",\n",
    "    \"n01828970\",\n",
    "    \"n02018795\",\n",
    "    \"n01695060\",\n",
    "    \"n01729322\",\n",
    "    \"n01677366\",\n",
    "    \"n01734418\",\n",
    "    \"n01843383\",\n",
    "    \"n01806143\",\n",
    "    \"n01773549\",\n",
    "    \"n01775062\",\n",
    "    \"n01728572\",\n",
    "    \"n01601694\",\n",
    "    \"n01978287\",\n",
    "    \"n01930112\",\n",
    "    \"n01739381\",\n",
    "    \"n01883070\",\n",
    "    \"n01774384\",\n",
    "    \"n02037110\",\n",
    "    \"n01795545\",\n",
    "    \"n02027492\",\n",
    "    \"n01531178\",\n",
    "    \"n01944390\",\n",
    "    \"n01494475\",\n",
    "    \"n01632458\",\n",
    "    \"n01698640\",\n",
    "    \"n01675722\",\n",
    "    \"n01877812\",\n",
    "    \"n01622779\",\n",
    "    \"n01910747\",\n",
    "    \"n01860187\",\n",
    "    \"n01796340\",\n",
    "    \"n01833805\",\n",
    "    \"n01685808\",\n",
    "    \"n01756291\",\n",
    "    \"n01514859\",\n",
    "    \"n01753488\",\n",
    "    \"n02058221\",\n",
    "    \"n01632777\",\n",
    "    \"n01644900\",\n",
    "    \"n02018207\",\n",
    "    \"n01664065\",\n",
    "    \"n02028035\",\n",
    "    \"n02012849\",\n",
    "    \"n01776313\",\n",
    "    \"n02077923\",\n",
    "    \"n01774750\",\n",
    "    \"n01742172\",\n",
    "    \"n01943899\",\n",
    "    \"n01798484\",\n",
    "    \"n02051845\",\n",
    "    \"n01824575\",\n",
    "    \"n02013706\",\n",
    "    \"n01955084\",\n",
    "    \"n01773157\",\n",
    "    \"n01665541\",\n",
    "    \"n01498041\",\n",
    "    \"n01978455\",\n",
    "    \"n01693334\",\n",
    "    \"n01950731\",\n",
    "    \"n01829413\",\n",
    "    \"n01514668\"]\n",
    "   return imagenet_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saas,sasas,lables=get_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(): \n",
    "    model = Transformer(config[\"d_model\"], config[\"ffn_hidden\"], config[\"num_heads\"], config[\"drop_prob\"], config[\"num_layers\"], config[\"kn_vocab_size\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model():\n",
    "\n",
    "    image_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\",output_hidden_states =True,output_attentions=True)\n",
    "    image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    #image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    return image_model,image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_model():\n",
    "\n",
    "    processor_audio = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "    model_audio = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", output_hidden_states =True,output_attentions=True)\n",
    "    \n",
    "\n",
    "    return model_audio,processor_audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_model():\n",
    "    text_model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\",output_hidden_states =True,output_attentions=True)\n",
    "    text_Tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return text_model ,text_Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_layer():\n",
    "    \n",
    "    image_layerno_1=10\n",
    "    image_layerno_2=9\n",
    "    print(\"image layer number\",image_layerno_1,image_layerno_2)\n",
    "    return image_layerno_1 ,image_layerno_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_validation(model, validation_ds,labels_all, device, print_msg, global_step, writer, num_examples=10):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    image_model,image_processor =get_image_model()\n",
    "    #model_audio,processor_audio =get_audio_model()\n",
    "    text_model,text_Tokenizer =get_text_model()\n",
    "    image100id=imageNet100id()\n",
    "    image_layerno_1,image_layerno_2= get_hidden_layer()\n",
    "\n",
    "    source_image = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "    realclass_count=0\n",
    "    number_inputs=0\n",
    "    classid=''\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    #batch_size=config['batch_size'] \n",
    "    batch_size=1\n",
    "   # batch_iterator = tqdm(validation_ds.iter(batch_size))\n",
    "    batch_iterator = tqdm(validation_ds)\n",
    "       # print(\"batch_iterator,batch_iterator\",batch_iterator)\n",
    "        #for index, row in df.iterrows():\n",
    "        #for batch in batch_iterator:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_iterator :\n",
    "            count += 1\n",
    "\n",
    "########################  image KQV extraction \n",
    "\n",
    "            paths=batch[\"path\"]\n",
    "            image_file = Dataset.from_dict({\"image\": paths}).cast_column(\"image\", Image())  \n",
    "            \n",
    "            image_model=image_model.to(device)\n",
    "\n",
    "                \n",
    "            image_1 = image_processor(images=image_file[0][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_outputs_1 = image_model(**image_1)\n",
    "            image_last_hidden_state_1 = image_outputs_1.last_hidden_state\n",
    "            image_pooled_output_1 = image_outputs_1.pooler_output  \n",
    "            image_layear_output_1=image_outputs_1.hidden_states[image_layerno_1],image_outputs_1.hidden_states[image_layerno_2]\n",
    "                  \n",
    "                  \n",
    "            image_2 = image_processor(images=image_file[1][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_2 = image_model(**image_2)\n",
    "            image_last_hidden_state_2 = image_outputs_2.last_hidden_state\n",
    "            image_pooled_output_2 = image_outputs_2.pooler_output  \n",
    "            image_layear_output_2=image_outputs_2.hidden_states[image_layerno_1],image_outputs_2.hidden_states[image_layerno_2]\n",
    "            \n",
    "            \n",
    "            image_3 = image_processor(images=image_file[2][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_3 = image_model(**image_3)\n",
    "            image_last_hidden_state_3 = image_outputs_3.last_hidden_state\n",
    "            image_pooled_output_3 = image_outputs_3.pooler_output  \n",
    "            image_layear_output_3=image_outputs_3.hidden_states[image_layerno_1],image_outputs_3.hidden_states[image_layerno_2]\n",
    "            \n",
    "            image_4 = image_processor(images=image_file[3][\"image\"], return_tensors=\"pt\").to(device)\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs_4 = image_model(**image_4)\n",
    "            image_last_hidden_state_4 = image_outputs_4.last_hidden_state\n",
    "            image_pooled_output_4 = image_outputs_4.pooler_output  \n",
    "            image_layear_output_4=image_outputs_4.hidden_states[image_layerno_1],image_outputs_4.hidden_states[image_layerno_2]\n",
    "\n",
    "\n",
    "            label=batch[\"label\"]\n",
    "          \n",
    "\n",
    "\n",
    "            inputs_text = text_Tokenizer(text=label[0], padding=True, return_tensors=\"pt\").to(device)\n",
    "            text_model= text_model.to(device)\n",
    "            text_outputs = text_model(**inputs_text)\n",
    "            text_pooled_output = text_outputs.pooler_output\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "  \n",
    "#############################################################################\n",
    "            text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "            text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "            text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "            _start = 0\n",
    "            _end = config[\"d_model\"]\n",
    "            text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "            text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "            text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "\n",
    "            #image model parameters \n",
    "            image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "            image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "            image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "            image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "            image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "            image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "\n",
    "            \n",
    "            scores_1 = [None] * 100\n",
    "            scores_2 = [None] * 100\n",
    "            scores_3 = [None] * 100\n",
    "            scores_4 = [None] * 100\n",
    "                  \n",
    "            for x in range(0, 100):\n",
    "                #print(\"text\",labels_all[image100id[x]])\n",
    "                inputs_text = text_Tokenizer(text=labels_all[image100id[x]], padding=True, return_tensors=\"pt\").to(device)\n",
    "                text_model= text_model.to(device)\n",
    "                text_outputs = text_model(**inputs_text)\n",
    "                text_pooled_output = text_outputs.pooler_output\n",
    "\n",
    "                text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "                proj_output_1=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_1, image_pooled_output_1).to(device)\n",
    "                proj_output_1 = proj_output_1.detach().cpu().numpy()\n",
    "                text_pooled_output =text_pooled_output.detach().cpu().numpy()\n",
    "      \n",
    "                scores_1[x] = np.dot(proj_output_1, text_pooled_output.T)/(norm(proj_output_1)*(norm(text_pooled_output))) \n",
    "                    \n",
    "                    \n",
    "                proj_output_2=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_2, image_pooled_output_2).to(device)  \n",
    "                      \n",
    "                    \n",
    "                proj_output_2 = proj_output_2.detach().cpu().numpy()\n",
    "                  \n",
    "                scores_2[x] = np.dot(proj_output_2, text_pooled_output.T)/(norm(proj_output_2)*(norm(text_pooled_output))) \n",
    "                \n",
    "                proj_output_3=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_3, image_pooled_output_3).to(device)  \n",
    "                      \n",
    "                    \n",
    "                proj_output_3 = proj_output_3.detach().cpu().numpy()\n",
    "                  \n",
    "                scores_3[x] = np.dot(proj_output_3, text_pooled_output.T)/(norm(proj_output_3)*(norm(text_pooled_output))) \n",
    "                    #scores_2[x]\n",
    "                    \n",
    "                proj_output_4=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n",
    "                                    image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_4, image_pooled_output_4).to(device)  \n",
    "                      \n",
    "                    \n",
    "                proj_output_4 = proj_output_4.detach().cpu().numpy()\n",
    "                  \n",
    "                scores_4[x] = np.dot(proj_output_4, text_pooled_output.T)/(norm(proj_output_4)*(norm(text_pooled_output))) \n",
    "                    \n",
    "                    \n",
    "                  #print(\"real_class\",np.argmax(scores_1),np.argmax(scores_2))\n",
    "            real_class=np.argmax(scores_1),np.argmax(scores_2),np.argmax(scores_3),np.argmax(scores_4)\n",
    "                \n",
    "           # print(\"real_class\",real_class[0],real_class[1],real_class[2],real_class[3])\n",
    "                  #print(\"scores\",np.argmax(appfor, axis=0),label)\n",
    "                \n",
    "              \n",
    "        \n",
    "            classid=str(batch[\"id\"][0]),str(batch[\"id\"][1]),str(batch[\"id\"][2]),str(batch[\"id\"][3])\n",
    "           # print(str(batch[\"id\"][0]),str(batch[\"id\"][1]),str(batch[\"id\"][2]),str(batch[\"id\"][3]))\n",
    "           # print(image100id[real_class[0]],image100id[real_class[1]],image100id[real_class[2]],image100id[real_class[3]])\n",
    "            for x in range(0, 4):\n",
    "              #  print(x)\n",
    "                if classid[x] == image100id[real_class[x]]:\n",
    "                        realclass_count=realclass_count+1\n",
    "                        number_inputs=number_inputs+1\n",
    "                        #print(\"correct\",realclass_count)\n",
    "                else:\n",
    "                        #  print(\"wwwwwwwwwww\",number_inputs)\n",
    "                        number_inputs=number_inputs+1\n",
    "\n",
    "\n",
    "                  \n",
    "\n",
    "                  #source_image.append(scores)\n",
    "            expected.append(1.00)\n",
    "            predicted.append(scores_1)\n",
    "            \n",
    "\n",
    "\n",
    "    Accuracy =(realclass_count / number_inputs*100 )\n",
    "\n",
    "    print(\"Accuracy\", Accuracy,realclass_count,number_inputs, \"%\")\n",
    "    return Accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "image layer number 7 6\n",
      "Training dataset size  130000\n",
      "No model to preload, starting from scratch\n",
      "Layers  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 6500/6500 [1:44:35<00:00,  1.04it/s, loss=3.671] \n",
      "Processing Epoch 01:  83%|████████▎ | 5376/6500 [1:18:10<16:20,  1.15it/s, loss=3.985] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 171\u001b[0m\n\u001b[1;32m    169\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m--> 171\u001b[0m train_model(config)\n",
      "Cell \u001b[0;32mIn[13], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     67\u001b[0m  \u001b[38;5;66;03m#dataset = Dataset.from_dict({\"image\": [\"path/to/image_1\", \"path/to/image_2\", ..., \"path/to/image_n\"]}).cast_column(\"image\", Image())\u001b[39;00m\n\u001b[1;32m     68\u001b[0m  \u001b[38;5;66;03m#dataset[0][\"image\"]\u001b[39;00m\n\u001b[1;32m     69\u001b[0m  \u001b[38;5;66;03m#print(\"maithri ranga1\",image_file)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m  iend \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m---> 71\u001b[0m  image \u001b[38;5;241m=\u001b[39m image_processor(images\u001b[38;5;241m=\u001b[39mimage_file[\u001b[38;5;241m0\u001b[39m:iend][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# image = image_processor(images=batch['path'], return_tensors=\"pt\").to(device)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m  image_model\u001b[38;5;241m=\u001b[39mimage_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2872\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2857\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2855\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2856\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2857\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2858\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2859\u001b[0m )\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:456\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    455\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m--> 456\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/formatting/formatting.py:228\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_batch(batch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/features/features.py:2081\u001b[0m, in \u001b[0;36mFeatures.decode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2078\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2080\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2081\u001b[0m         [\n\u001b[1;32m   2082\u001b[0m             decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   2083\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2085\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2086\u001b[0m         ]\n\u001b[1;32m   2087\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2088\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/features/features.py:2082\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2078\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2080\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2081\u001b[0m         [\n\u001b[0;32m-> 2082\u001b[0m             decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   2083\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2085\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2086\u001b[0m         ]\n\u001b[1;32m   2087\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2088\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/features/features.py:1400\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[0;32m-> 1400\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode_example(obj, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/features/image.py:169\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_local_path(path):\n\u001b[0;32m--> 169\u001b[0m         image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m         source_url \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3225\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m   3229\u001b[0m preinit()\n\u001b[1;32m   3231\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "   #device=\"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader,val_dataloader,lablesall= get_ds()\n",
    "    image_model,image_processor =get_image_model()\n",
    "    #model_audio,processor_audio =get_audio_model()\n",
    "    text_model,text_Tokenizer =get_text_model()\n",
    "    image_layerno_1,image_layerno_2= get_hidden_layer()\n",
    "  \n",
    "    print(\"Training dataset size \", len(train_dataloader )*config['batch_size'] )  \n",
    "\n",
    "    model = get_model().to(device)\n",
    "\n",
    "     # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "\n",
    "   # optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    value_opt = torch.optim.Adam(params = model.parameters(), lr = config['lr'])\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename, map_location='cpu')\n",
    "        model.load_state_dict(state)\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    #loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)   \n",
    "    \n",
    "\n",
    "\n",
    "   # abc=torch.rand(1, 768).to(device)\n",
    "    print(\"Layers \",config[\"num_layers\"])\n",
    "    \n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        #print(\"epoch\",epoch)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        #train_dataset_both ,val_dataset_both ,train_dataloader,val_dataloader= get_ds()\n",
    "        model.train()\n",
    "        batch_size=config['batch_size'] \n",
    "  \n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "              \n",
    "########################  image KQV extraction \n",
    "            \n",
    "\n",
    "            paths=batch[\"path\"]\n",
    "            image_file = Dataset.from_dict({\"image\": paths}).cast_column(\"image\", Image())  \n",
    "            #dataset = Dataset.from_dict({\"image\": [\"path/to/image_1\", \"path/to/image_2\", ..., \"path/to/image_n\"]}).cast_column(\"image\", Image())\n",
    "            #dataset[0][\"image\"]\n",
    "            #print(\"maithri ranga1\",image_file)\n",
    "            iend = config['batch_size'] \n",
    "            image = image_processor(images=image_file[0:iend][\"image\"], return_tensors=\"pt\").to(device)\n",
    "\n",
    "           # image = image_processor(images=batch['path'], return_tensors=\"pt\").to(device)\n",
    "\n",
    "            image_model=image_model.to(device)\n",
    "            image_outputs = image_model(**image)\n",
    "            image_last_hidden_state = image_outputs.last_hidden_state\n",
    "            image_pooled_output = image_outputs.pooler_output  \n",
    "           # print(\"image_pooled_output\",image_pooled_output.shape)\n",
    "           # batch_nummber+= 1\n",
    "         \n",
    "            image_layear_output=image_outputs.hidden_states[image_layerno_1],image_outputs.hidden_states[image_layerno_2]\n",
    "\n",
    "            \n",
    "\n",
    "            label=batch[\"label\"]\n",
    "\n",
    "           \n",
    "            #print(\"imageddddddddddddddddddddddd\",label,flabel)\n",
    "            inputs_text = text_Tokenizer(text=label, padding=True, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            text_model= text_model.to(device)\n",
    "            text_outputs = text_model(**inputs_text)\n",
    "            \n",
    "            text_pooled_output = text_outputs.pooler_output\n",
    "           # print(\"imageddddddddddddddddddddddd\",text_pooled_output.shape)\n",
    "            text_layear_output=text_outputs.hidden_states[image_layerno_1],text_outputs.hidden_states[image_layerno_2]\n",
    "            \n",
    "\n",
    "\n",
    "            #text model parameters \n",
    "            text_query_bias=text_model.encoder.layer[image_layerno_1].attention.attn.q.bias,text_model.encoder.layer[image_layerno_2].attention.attn.q.bias \n",
    "            text_key_bias=text_model.encoder.layer[image_layerno_1].attention.attn.k.bias,text_model.encoder.layer[image_layerno_2].attention.attn.k.bias\n",
    "            text_value_bias=text_model.encoder.layer[image_layerno_1].attention.attn.v.bias,text_model.encoder.layer[image_layerno_2].attention.attn.v.bias\n",
    "            _start = 0\n",
    "            _end = config[\"d_model\"]\n",
    "            text_query_weight=text_model.encoder.layer[image_layerno_1].attention.attn.q.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.q.weight[_start:_end, :]\n",
    "            text_key_weight=text_model.encoder.layer[image_layerno_1].attention.attn.k.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.k.weight[_start:_end, :]\n",
    "            text_value_weight=text_model.encoder.layer[image_layerno_1].attention.attn.v.weight[_start:_end, :],text_model.encoder.layer[image_layerno_2].attention.attn.v.weight[_start:_end, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #image model parameters \n",
    "            image_query_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.bias\n",
    "            image_key_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.bias\n",
    "            image_value_bias=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.bias,image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.bias\n",
    "\n",
    "            image_query_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.q_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.q_proj.weight[_start:_end, :]\n",
    "            image_key_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.k_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.k_proj.weight[_start:_end, :]\n",
    "            image_value_weight=image_model.vision_model.encoder.layers[image_layerno_1].self_attn.v_proj.weight[_start:_end, :],image_model.vision_model.encoder.layers[image_layerno_2].self_attn.v_proj.weight[_start:_end, :]\n",
    "            \n",
    "            #proj_output=model(config['d_model'], config['ffn_hidden'], config['num_heads'], config['drop_prob'], config['num_layers'], config['kn_vocab_size'], model_audio, image_model, outputs_audio, image_outputs)\n",
    "            proj_output=model(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias,image_key_bias,image_value_bias, image_query_weight,image_key_weight,image_value_weight, \n",
    "                              text_layear_output, image_layear_output,image_pooled_output).to(device)\n",
    "           # out_image = Transformer(audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight, image_model, audio_layear_output, image_layear_output)\n",
    "            text_pooled_output=text_pooled_output/text_pooled_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      \n",
    "            #image_pooled_output=torch.squeeze(image_pooled_output,dim=0).to(device)\n",
    "  \n",
    "            proj_output=proj_output/proj_output.norm(dim=-1, keepdim=True)\n",
    "            image_last_hidden_state=image_last_hidden_state/image_last_hidden_state.norm(dim=-1, keepdim=True)\n",
    "            image_pooled_output=image_pooled_output/image_pooled_output.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1,768), text_pooled_output .view(-1,768))   # compare with pooled_output image to audio\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        \n",
    "            global_step += 1\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Layers  1\n",
      "Training dataset size  1250\n",
      "weights/tmodel_00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image layer number 7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1250 [00:16<38:40,  1.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_filename , map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m tAccuracy \u001b[38;5;241m=\u001b[39m run_validation(model, val_dataset_both,labels_all, device,  \u001b[38;5;28;01mlambda\u001b[39;00m msg: \u001b[38;5;28mprint\u001b[39m(msg), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m)\n\u001b[1;32m     29\u001b[0m Accuracy\u001b[38;5;241m.\u001b[39mappend(tAccuracy)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge\u001b[39m\u001b[38;5;124m\"\u001b[39m,tAccuracy)\n",
      "Cell \u001b[0;32mIn[12], line 152\u001b[0m, in \u001b[0;36mrun_validation\u001b[0;34m(model, validation_ds, labels_all, device, print_msg, global_step, writer, num_examples)\u001b[0m\n\u001b[1;32m    149\u001b[0m scores_3[x] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(proj_output_3, text_pooled_output\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m/\u001b[39m(norm(proj_output_3)\u001b[38;5;241m*\u001b[39m(norm(text_pooled_output))) \n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m#scores_2[x]\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m proj_output_4\u001b[38;5;241m=\u001b[39mmodel(batch_size,text_query_bias,text_key_bias,text_value_bias,text_query_weight,text_key_weight,text_value_weight,image_query_bias, image_key_bias,image_value_bias,\n\u001b[1;32m    153\u001b[0m                     image_query_weight,image_key_weight,image_value_weight, text_layear_output, image_layear_output_4, image_pooled_output_4)\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[1;32m    156\u001b[0m proj_output_4 \u001b[38;5;241m=\u001b[39m proj_output_4\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    158\u001b[0m scores_4[x] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(proj_output_4, text_pooled_output\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m/\u001b[39m(norm(proj_output_4)\u001b[38;5;241m*\u001b[39m(norm(text_pooled_output))) \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SAFFE/model.py:475\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, batch_size, audio_query_bias, audio_key_bias, audio_value_bias, audio_query_weight, audio_key_weight, audio_value_weight, image_query_bias, image_key_bias, image_value_bias, image_query_weight, image_key_weight, image_value_weight, outputs_audio, image_outputs, image_pooled_output)\u001b[0m\n\u001b[1;32m    472\u001b[0m  \u001b[38;5;66;03m############################ image to Audio\u001b[39;00m\n\u001b[1;32m    473\u001b[0m         mask\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m         decoder_image_10\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_image( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_a_10, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_a_10, mask,audio_query_bias[\u001b[38;5;241m0\u001b[39m],audio_key_bias[\u001b[38;5;241m0\u001b[39m],audio_value_bias[\u001b[38;5;241m0\u001b[39m],audio_query_weight[\u001b[38;5;241m0\u001b[39m],audio_key_weight[\u001b[38;5;241m0\u001b[39m],audio_value_weight[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    476\u001b[0m                                               image_query_bias[\u001b[38;5;241m0\u001b[39m],image_key_bias[\u001b[38;5;241m0\u001b[39m],image_value_bias[\u001b[38;5;241m0\u001b[39m],image_query_weight[\u001b[38;5;241m0\u001b[39m],image_key_weight[\u001b[38;5;241m0\u001b[39m],image_value_weight[\u001b[38;5;241m0\u001b[39m],image_outputs[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_a_10_2)\n\u001b[1;32m    477\u001b[0m         decoder_image_9\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_image( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_a_9, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_a_9, mask,audio_query_bias[\u001b[38;5;241m1\u001b[39m],audio_key_bias[\u001b[38;5;241m1\u001b[39m],audio_value_bias[\u001b[38;5;241m1\u001b[39m],audio_query_weight[\u001b[38;5;241m1\u001b[39m],audio_key_weight[\u001b[38;5;241m1\u001b[39m],audio_value_weight[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    478\u001b[0m                                              image_query_bias[\u001b[38;5;241m1\u001b[39m],image_key_bias[\u001b[38;5;241m1\u001b[39m],image_value_bias[\u001b[38;5;241m1\u001b[39m],image_query_weight[\u001b[38;5;241m1\u001b[39m],image_key_weight[\u001b[38;5;241m1\u001b[39m],image_value_weight[\u001b[38;5;241m1\u001b[39m],image_outputs[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_a_9_2)\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;66;03m#print(\"##########image to Audio#########################################\")\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m################################################\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SAFFE/model.py:431\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, y, mask, audio_query_bias, audio_key_bias, audio_value_bias, audio_query_weight, audio_key_weight, audio_value_weight, image_query_bias, image_key_bias, image_value_bias, image_query_weight, image_key_weight, image_value_weight, image_outputs, constant_y)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y,mask,audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight,image_query_bias, image_key_bias,image_value_bias,\n\u001b[1;32m    427\u001b[0m             image_query_weight,image_key_weight,image_value_weight,image_outputs,constant_y):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m#x : 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m#y : 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m#mask : 200 x 200\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x, y, mask,audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight, image_query_bias, image_key_bias,image_value_bias,\n\u001b[1;32m    432\u001b[0m             image_query_weight,image_key_weight,image_value_weight,image_outputs,constant_y)\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SAFFE/model.py:416\u001b[0m, in \u001b[0;36mSequentialDecoder.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    414\u001b[0m x, y, mask,audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight,image_query_bias, image_key_bias,image_value_bias,image_query_weight,image_key_weight,image_value_weight,image_outputs,constant_y\u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 416\u001b[0m     y \u001b[38;5;241m=\u001b[39m module(x, y, mask,audio_query_bias,audio_key_bias,audio_value_bias,audio_query_weight,audio_key_weight,audio_value_weight,image_query_bias, image_key_bias,image_value_bias,\n\u001b[1;32m    417\u001b[0m         image_query_weight,image_key_weight,image_value_weight,image_outputs,constant_y) \u001b[38;5;66;03m#30 x 200 x 512\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SAFFE/model.py:389\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, y, decoder_mask, audio_query_bias, audio_key_bias, audio_value_bias, audio_query_weight, audio_key_weight, audio_value_weight, image_query_bias, image_key_bias, image_value_bias, image_query_weight, image_key_weight, image_value_weight, image_outputs, constant_y)\u001b[0m\n\u001b[1;32m    387\u001b[0m _y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;66;03m# 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m#print(\"MASKED SELF ATTENTION\")\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(y,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask, image_query_bias,image_key_bias,image_value_bias,image_query_weight,image_key_weight,image_value_weight,image_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_outputs) \u001b[38;5;66;03m# 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m#print(\"DROP OUT 1\")\u001b[39;00m\n\u001b[1;32m    391\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(y) \u001b[38;5;66;03m# 30 x 200 x 512\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/SAFFE/model.py:256\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask, image_query_bias, image_key_bias, image_value_bias, image_query_weight, image_key_weight, image_value_weight, image_outputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m values, attention \u001b[38;5;241m=\u001b[39m scaled_dot_product(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, mask) \u001b[38;5;66;03m# values: 30 x 8 x 200 x 64\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m#print(f\"values: {values.size()}, attention:{attention.size()}\")\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m#values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim) # 30 x 200 x 512\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m values\u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, sequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m#print(f\"values after reshaping: {values.size()}\")\u001b[39;00m\n\u001b[1;32m    258\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer(values) \u001b[38;5;66;03m# 30 x 200 x 512\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataset_both ,val_dataset_both,labels_all = get_ds()\n",
    "print(\"Layers \",config[\"num_layers\"])\n",
    "print(\"Training dataset size \", len(val_dataset_both ))\n",
    "#val_dataset_both2 =val_dataset_both.shard(num_shards=2, index=0)\n",
    "model = get_model().to(device)\n",
    "Accuracy = []\n",
    "for number in range(0, 40, 1):\n",
    "    if number < 10: \n",
    "        str(number)\n",
    "        number= \"0\"+ str(number)\n",
    "    else:\n",
    "        number=str(number)\n",
    "    cwd2 = \"tmodel_\" + number + \".pt\"\n",
    "    cwd = \"weights\"\n",
    "    os.path.join(cwd,cwd2)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    model_filename =os.path.join(cwd,cwd2)\n",
    "    print(model_filename)\n",
    "    state = torch.load(model_filename , map_location='cpu')\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    tAccuracy = run_validation(model, val_dataset_both,labels_all, device,  lambda msg: print(msg), 0, None, num_examples=2500)\n",
    "    Accuracy.append(tAccuracy)\n",
    "    print(\"Merge\",tAccuracy)\n",
    "    with open('output.txt', 'a') as testwritefile:\n",
    "        testwritefile.write(str(number))\n",
    "        testwritefile.write(\"layer accuracy \")\n",
    "        testwritefile.write(str(tAccuracy ))\n",
    "        testwritefile.write(\"\\n \")\n",
    "    print(Accuracy)\n",
    "max_value_index = np.argmax(Accuracy)\n",
    "Maximum_Accuracy = Accuracy[max_value_index]\n",
    "print(\"Maximum_Accuracy:\", Maximum_Accuracy ,\"Maximum Accuracy epoch:\",max_value_index )\n",
    "with open('output.txt', 'a') as testwritefile:\n",
    "        testwritefile.write(\"Maximum_Accuracy:\")\n",
    "        testwritefile.write(str(Maximum_Accuracy))\n",
    "        testwritefile.write(\"Maximum Accuracy epoch:\")\n",
    "        testwritefile.write(str(max_value_index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
